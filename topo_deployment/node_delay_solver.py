# node_delay_solver.py

import torch
import numpy as np
import matplotlib.pyplot as plt
import warnings


class NodeDelaySolver:
    """
    åŸºäºèŠ‚ç‚¹çš„å»¶è¿Ÿæ±‚è§£å™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    
    æ–°å¢åŠŸèƒ½ï¼š
    1. ç‰©ç†çº¦æŸéªŒè¯ä¸ä¿®æ­£ï¼ˆæ–¹æ¡ˆ Aï¼šç‰©ç†ä¸‹ç•Œï¼‰
    2. ç»“æœéªŒè¯è¯Šæ–­
    3. æ—©åœæœºåˆ¶
    4. å­¦ä¹ ç‡è°ƒåº¦
    
    ç‰©ç†æ¨¡å‹ï¼š
    - r = M @ dï¼Œå…¶ä¸­ d æ˜¯èŠ‚ç‚¹å»¶è¿Ÿå‘é‡
    - ä¼˜åŒ–ç›®æ ‡ï¼šæ‰¾åˆ°åˆç†çš„èŠ‚ç‚¹å»¶è¿Ÿ dï¼Œä½¿å¾—é¢„æµ‹å»¶è¿Ÿä¸å®é™…æµ‹é‡æ¥è¿‘
    """
    
    def __init__(self, topo_num, prob_num, unit_delay=12.0, lr=0.01, max_iter=3000,
                 early_stop_patience=100, early_stop_threshold=1e-5):
        """
        å‚æ•°ï¼š
            topo_num: æ‹“æ‰‘ç¼–å·
            prob_num: æ¢æµ‹æ¬¡æ•°
            unit_delay: ç†è®ºå•ä½å»¶è¿Ÿï¼ˆÎ¼sï¼‰
            lr: åˆå§‹å­¦ä¹ ç‡
            max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
            early_stop_patience: æ—©åœç­‰å¾…æ­¥æ•°
            early_stop_threshold: æ—©åœé˜ˆå€¼ï¼ˆç›¸å¯¹æ”¹è¿›ï¼‰
        """
        self.topo_num = topo_num
        self.prob_num = prob_num
        self.unit_delay = unit_delay
        self.lr = lr
        self.max_iter = max_iter
        self.early_stop_patience = early_stop_patience
        self.early_stop_threshold = early_stop_threshold
        
        # è¾“å‡ºè·¯å¾„
        base_path = f"/home/retr0/Project/TopologyObfu/CritiPro/topo_deployment/data/output_file"
        self.node_delay_path = f"{base_path}/{topo_num}_{prob_num}_node_delays.txt"
        self.predicted_delay_path = f"{base_path}/{topo_num}_{prob_num}_predicted_delays.txt"
        self.convergence_png_path = f"{base_path}/{topo_num}_{prob_num}_node_delay_convergence.png"
        self.diagnostic_report_path = f"{base_path}/{topo_num}_{prob_num}_diagnostic_report.txt"
    
    def read_matrix(self, file_path):
        """è¯»å–çŸ©é˜µæ–‡ä»¶"""
        return torch.tensor(np.loadtxt(file_path), dtype=torch.float32)
    
    def read_vector(self, file_path):
        """è¯»å–å‘é‡æ–‡ä»¶"""
        return torch.tensor(np.loadtxt(file_path).reshape(-1, 1), dtype=torch.float32)
    
    def validate_physical_constraints(self, M, r_measured):
        """
        ç‰©ç†çº¦æŸéªŒè¯ä¸ä¿®æ­£ï¼ˆæ–¹æ¡ˆ Aï¼šç‰©ç†ä¸‹ç•Œï¼‰
        
        æ£€æŸ¥æ¯ä¸ªæ¥æ”¶å™¨å¯¹çš„æµ‹é‡å»¶è¿Ÿæ˜¯å¦ >= ç‰©ç†ä¸‹ç•Œ
        ç‰©ç†ä¸‹ç•Œ = å…±äº«èŠ‚ç‚¹æ•° Ã— unit_delay
        
        å‚æ•°ï¼š
            M: è·¯ç”±çŸ©é˜µ (num_pairs, num_nodes)
            r_measured: æµ‹é‡å»¶è¿Ÿå‘é‡ (num_pairs, 1)
            
        è¿”å›ï¼š
            corrected_delays: ä¿®æ­£åçš„å»¶è¿Ÿå‘é‡
            violations: è¿åç‰©ç†çº¦æŸçš„é…å¯¹ç´¢å¼•
        """
        # è®¡ç®—æ¯å¯¹çš„å…±äº«èŠ‚ç‚¹æ•°
        shared_counts = torch.sum(M, dim=1, keepdim=True)  # (num_pairs, 1)
        
        # è®¡ç®—ç‰©ç†ä¸‹ç•Œï¼ˆæ–¹æ¡ˆ Aï¼‰
        physical_lower_bound = shared_counts * self.unit_delay
        
        # æ£€æŸ¥è¿åæƒ…å†µ
        violations = (r_measured < physical_lower_bound).squeeze()
        num_violations = torch.sum(violations).item()
        
        # ä¿®æ­£ï¼šå°†å°äºç‰©ç†ä¸‹ç•Œçš„å»¶è¿Ÿæå‡åˆ°ä¸‹ç•Œ
        corrected_delays = torch.maximum(r_measured, physical_lower_bound)
        
        # æ‰“å°éªŒè¯ä¿¡æ¯
        print(f"\n{'='*70}")
        print(f"ç‰©ç†çº¦æŸéªŒè¯ï¼ˆæ–¹æ¡ˆ Aï¼šç‰©ç†ä¸‹ç•Œï¼‰")
        print(f"{'='*70}")
        print(f"æ£€æŸ¥é¡¹ç›®: æµ‹é‡å»¶è¿Ÿ >= å…±äº«èŠ‚ç‚¹æ•° Ã— å•ä½å»¶è¿Ÿ")
        print(f"å•ä½å»¶è¿Ÿ: {self.unit_delay} Î¼s")
        print(f"æ€»é…å¯¹æ•°: {len(r_measured)}")
        print(f"è¿åçº¦æŸçš„é…å¯¹æ•°: {num_violations}")
        
        if num_violations > 0:
            print(f"\nâš ï¸  å‘ç° {num_violations} ä¸ªé…å¯¹çš„æµ‹é‡å»¶è¿Ÿä½äºç‰©ç†ä¸‹ç•Œ")
            print(f"{'é…å¯¹ç´¢å¼•':<10} {'æµ‹é‡å»¶è¿Ÿ(Î¼s)':<15} {'ç‰©ç†ä¸‹ç•Œ(Î¼s)':<15} {'ä¿®æ­£å(Î¼s)':<15}")
            print(f"{'-'*70}")
            
            violation_indices = torch.where(violations)[0]
            for idx in violation_indices:
                idx_val = idx.item()
                measured = r_measured[idx_val].item()
                lower = physical_lower_bound[idx_val].item()
                corrected = corrected_delays[idx_val].item()
                print(f"{idx_val:<10} {measured:>13.4f}  {lower:>13.4f}  {corrected:>13.4f}")
            
            print(f"\nâœ“ å·²å°†è¿™äº›é…å¯¹çš„ç›®æ ‡å»¶è¿Ÿä¿®æ­£ä¸ºç‰©ç†ä¸‹ç•Œ")
        else:
            print(f"âœ“ æ‰€æœ‰é…å¯¹çš„æµ‹é‡å»¶è¿Ÿå‡æ»¡è¶³ç‰©ç†çº¦æŸ")
        
        print(f"{'='*70}\n")
        
        return corrected_delays, violation_indices if num_violations > 0 else None
    
    def should_early_stop(self, loss_history):
        """
        æ—©åœåˆ¤æ–­
        
        å¦‚æœæœ€è¿‘ patience æ¬¡è¿­ä»£çš„æŸå¤±ç›¸å¯¹æ”¹è¿› < thresholdï¼Œåˆ™åœæ­¢
        
        å‚æ•°ï¼š
            loss_history: æŸå¤±å†å²åˆ—è¡¨
            
        è¿”å›ï¼š
            bool: æ˜¯å¦åº”è¯¥åœæ­¢
        """
        if len(loss_history) < self.early_stop_patience:
            return False
        
        recent_losses = loss_history[-self.early_stop_patience:]
        
        # è®¡ç®—ç›¸å¯¹æ”¹è¿›
        max_loss = max(recent_losses)
        min_loss = min(recent_losses)
        
        if max_loss == 0:
            return True
        
        relative_improvement = (max_loss - min_loss) / max_loss
        
        return relative_improvement < self.early_stop_threshold
    
    def solve(self, M_file, r_file):
        """
        æ±‚è§£èŠ‚ç‚¹å»¶è¿Ÿ
        
        å‚æ•°ï¼š
            M_file: è·¯ç”±çŸ©é˜µæ–‡ä»¶è·¯å¾„
            r_file: æµ‹é‡å»¶è¿Ÿå‘é‡æ–‡ä»¶è·¯å¾„
            
        è¿”å›ï¼š
            d: ä¼˜åŒ–åçš„èŠ‚ç‚¹å»¶è¿Ÿå‘é‡
            r_pred: é¢„æµ‹çš„å»¶è¿Ÿå‘é‡
        """
        # è¯»å–æ•°æ®
        M = self.read_matrix(M_file)  # (num_pairs, num_nodes)
        r_measured = self.read_vector(r_file)  # (num_pairs, 1)
        
        num_nodes = M.shape[1]
        num_pairs = M.shape[0]
        
        # ========== æ­¥éª¤ 1: ç‰©ç†çº¦æŸéªŒè¯ä¸ä¿®æ­£ ==========
        r_target, violations = self.validate_physical_constraints(M, r_measured)
        
        # åˆå§‹åŒ–èŠ‚ç‚¹å»¶è¿Ÿï¼ˆæ¥è¿‘ç†è®ºå€¼ï¼‰
        d = torch.nn.Parameter(
            torch.full((num_nodes, 1), self.unit_delay, dtype=torch.float32),
            requires_grad=True
        )
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam([d], lr=self.lr)
        
        # ========== æ­¥éª¤ 2: å­¦ä¹ ç‡è°ƒåº¦å™¨ ==========
        # å½“æŸå¤±ä¸å†ä¸‹é™æ—¶ï¼Œé™ä½å­¦ä¹ ç‡
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, 
            mode='min', 
            factor=0.5,          # å­¦ä¹ ç‡è¡°å‡å› å­
            patience=50,         # ç­‰å¾…æ­¥æ•°
            verbose=True,
            min_lr=1e-6          # æœ€å°å­¦ä¹ ç‡
        )
        
        # è®°å½•æŸå¤±
        loss_history = []
        
        # æ‰“å°ä¼˜åŒ–å¼€å§‹ä¿¡æ¯
        print(f"\n{'='*70}")
        print(f"å¼€å§‹ä¼˜åŒ–èŠ‚ç‚¹å»¶è¿Ÿ (æ‹“æ‰‘ {self.topo_num}, æ¢æµ‹ {self.prob_num})")
        print(f"{'='*70}")
        print(f"è·¯ç”±çŸ©é˜µå½¢çŠ¶: {M.shape}")
        print(f"æµ‹é‡å»¶è¿Ÿå‘é‡é•¿åº¦: {num_pairs}")
        print(f"èŠ‚ç‚¹æ•°é‡: {num_nodes}")
        print(f"ç†è®ºå•ä½å»¶è¿Ÿ: {self.unit_delay} Î¼s")
        print(f"åˆå§‹å­¦ä¹ ç‡: {self.lr}")
        print(f"æœ€å¤§è¿­ä»£æ¬¡æ•°: {self.max_iter}")
        print(f"æ—©åœç­‰å¾…æ­¥æ•°: {self.early_stop_patience}")
        print(f"{'='*70}\n")
        
        # ========== æ­¥éª¤ 3: ä¼˜åŒ–å¾ªç¯ï¼ˆå¸¦æ—©åœï¼‰ ==========
        early_stopped = False
        final_step = self.max_iter
        
        for step in range(self.max_iter):
            optimizer.zero_grad()
            
            # è®¡ç®—é¢„æµ‹å»¶è¿Ÿ
            r_pred = M @ d  # (num_pairs, 1)
            
            # æŸå¤±å‡½æ•°
            # 1. é¢„æµ‹è¯¯å·®ï¼ˆä¸»è¦ç›®æ ‡ï¼Œä½¿ç”¨ä¿®æ­£åçš„ç›®æ ‡ï¼‰
            loss_fit = torch.mean((r_pred - r_target) ** 2)
            
            # 2. æ­£åˆ™åŒ–ï¼šèŠ‚ç‚¹å»¶è¿Ÿåº”æ¥è¿‘ç†è®ºå€¼
            loss_reg = 0.01 * torch.mean((d - self.unit_delay) ** 2)
            
            # 3. çº¦æŸï¼šèŠ‚ç‚¹å»¶è¿Ÿåº”ä¸ºæ­£ä¸”åœ¨åˆç†èŒƒå›´å†…
            loss_constraint = torch.sum(torch.relu(-d)) + \
                             torch.sum(torch.relu(d - self.unit_delay * 5))
            
            # æ€»æŸå¤±
            total_loss = loss_fit + loss_reg + loss_constraint
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            optimizer.step()
            
            # æŠ•å½±åˆ°å¯è¡ŒåŸŸ
            with torch.no_grad():
                d.data = torch.clamp(d.data, 0.1, self.unit_delay * 5)
            
            # è®°å½•æŸå¤±
            loss_history.append(total_loss.item())
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step(total_loss)
            
            # æ‰“å°è¿›åº¦
            if step % 100 == 0:
                rmse = torch.sqrt(loss_fit).item()
                current_lr = optimizer.param_groups[0]['lr']
                print(f"Step {step:4d}: Loss = {total_loss.item():.6f}, "
                      f"RMSE = {rmse:.4f} Î¼s, LR = {current_lr:.6f}")
            
            # ========== æ—©åœæ£€æŸ¥ ==========
            if self.should_early_stop(loss_history):
                print(f"\nğŸ›‘ æ—©åœè§¦å‘äºç¬¬ {step} æ­¥")
                print(f"   åŸå› : æœ€è¿‘ {self.early_stop_patience} æ­¥æŸå¤±æ”¹è¿› < {self.early_stop_threshold}")
                early_stopped = True
                final_step = step
                break
        
        # æœ€ç»ˆç»“æœ
        d_final = d.detach().numpy()
        r_pred_final = (M @ d).detach().numpy()
        
        # ========== æ­¥éª¤ 4: ç»“æœéªŒè¯ä¸è¯Šæ–­ ==========
        diagnostic_info = self.validate_solution(
            d_final, M.numpy(), r_target.numpy(), r_measured.numpy(), r_pred_final
        )
        
        # è®¡ç®—æœ€ç»ˆè¯¯å·®
        errors = r_measured.numpy() - r_pred_final
        rmse = np.sqrt(np.mean(errors ** 2))
        mae = np.mean(np.abs(errors))
        
        print(f"\n{'='*70}")
        print(f"ä¼˜åŒ–å®Œæˆ")
        print(f"{'='*70}")
        print(f"å®é™…è¿­ä»£æ¬¡æ•°: {final_step}")
        if early_stopped:
            print(f"æ—©åœçŠ¶æ€: æ˜¯ï¼ˆèŠ‚çœ {self.max_iter - final_step} æ¬¡è¿­ä»£ï¼‰")
        else:
            print(f"æ—©åœçŠ¶æ€: å¦ï¼ˆè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼‰")
        print(f"æœ€ç»ˆ RMSE: {rmse:.4f} Î¼s")
        print(f"å¹³å‡ç»å¯¹è¯¯å·®: {mae:.4f} Î¼s")
        print(f"æœ€å¤§ç»å¯¹è¯¯å·®: {np.max(np.abs(errors)):.4f} Î¼s")
        print(f"{'='*70}\n")
        
        # ä¿å­˜ç»“æœ
        np.savetxt(self.node_delay_path, d_final, fmt="%.6f")
        np.savetxt(self.predicted_delay_path, r_pred_final, fmt="%.6f")
        print(f"èŠ‚ç‚¹å»¶è¿Ÿå·²ä¿å­˜åˆ°: {self.node_delay_path}")
        print(f"é¢„æµ‹å»¶è¿Ÿå·²ä¿å­˜åˆ°: {self.predicted_delay_path}")
        
        # ä¿å­˜è¯Šæ–­æŠ¥å‘Š
        self._save_diagnostic_report(diagnostic_info, early_stopped, final_step)
        
        # ç»˜åˆ¶æ”¶æ•›æ›²çº¿
        self._plot_convergence(loss_history)
        
        # æ‰“å°èŠ‚ç‚¹å»¶è¿Ÿè¯¦æƒ…
        self._print_node_delays(d_final)
        
        return d_final, r_pred_final
    
    def validate_solution(self, d, M, r_target, r_measured, r_pred):
        """
        ç»“æœéªŒè¯ä¸è¯Šæ–­
        
        æ£€æŸ¥é¡¹ï¼š
        1. èŠ‚ç‚¹å»¶è¿Ÿæ˜¯å¦åœ¨åˆç†èŒƒå›´
        2. é¢„æµ‹å»¶è¿Ÿè¯¯å·®æ˜¯å¦å¯æ¥å—
        3. æ˜¯å¦å­˜åœ¨å¼‚å¸¸èŠ‚ç‚¹
        4. ç‰©ç†çº¦æŸæ»¡è¶³æƒ…å†µ
        
        å‚æ•°ï¼š
            d: èŠ‚ç‚¹å»¶è¿Ÿå‘é‡
            M: è·¯ç”±çŸ©é˜µ
            r_target: ç›®æ ‡å»¶è¿Ÿï¼ˆä¿®æ­£åï¼‰
            r_measured: åŸå§‹æµ‹é‡å»¶è¿Ÿ
            r_pred: é¢„æµ‹å»¶è¿Ÿ
            
        è¿”å›ï¼š
            dict: è¯Šæ–­ä¿¡æ¯
        """
        warnings_list = []
        
        print(f"\n{'='*70}")
        print(f"ç»“æœéªŒè¯ä¸è¯Šæ–­")
        print(f"{'='*70}\n")
        
        # ========== æ£€æŸ¥ 1: èŠ‚ç‚¹å»¶è¿ŸèŒƒå›´ ==========
        d_flat = d.flatten()
        min_delay = 0.5 * self.unit_delay
        max_delay = 5.0 * self.unit_delay
        
        outliers_low = np.where(d_flat < min_delay)[0]
        outliers_high = np.where(d_flat > max_delay)[0]
        
        print(f"[æ£€æŸ¥ 1] èŠ‚ç‚¹å»¶è¿ŸèŒƒå›´")
        print(f"  åˆç†èŒƒå›´: [{min_delay:.2f}, {max_delay:.2f}] Î¼s")
        print(f"  å®é™…èŒƒå›´: [{np.min(d_flat):.2f}, {np.max(d_flat):.2f}] Î¼s")
        
        if len(outliers_low) > 0:
            warnings_list.append(f"âš ï¸  {len(outliers_low)} ä¸ªèŠ‚ç‚¹å»¶è¿Ÿè¿‡ä½ (< {min_delay:.2f} Î¼s): {outliers_low}")
            print(f"  âš ï¸  è¿‡ä½èŠ‚ç‚¹: {outliers_low}")
        
        if len(outliers_high) > 0:
            warnings_list.append(f"âš ï¸  {len(outliers_high)} ä¸ªèŠ‚ç‚¹å»¶è¿Ÿè¿‡é«˜ (> {max_delay:.2f} Î¼s): {outliers_high}")
            print(f"  âš ï¸  è¿‡é«˜èŠ‚ç‚¹: {outliers_high}")
        
        if len(outliers_low) == 0 and len(outliers_high) == 0:
            print(f"  âœ“ æ‰€æœ‰èŠ‚ç‚¹å»¶è¿Ÿåœ¨åˆç†èŒƒå›´å†…")
        
        # ========== æ£€æŸ¥ 2: é¢„æµ‹è¯¯å·® ==========
        errors = r_measured.flatten() - r_pred.flatten()
        rmse = np.sqrt(np.mean(errors ** 2))
        mae = np.mean(np.abs(errors))
        max_error = np.max(np.abs(errors))
        
        print(f"\n[æ£€æŸ¥ 2] é¢„æµ‹å»¶è¿Ÿè¯¯å·®")
        print(f"  RMSE: {rmse:.4f} Î¼s")
        print(f"  MAE: {mae:.4f} Î¼s")
        print(f"  æœ€å¤§è¯¯å·®: {max_error:.4f} Î¼s")
        
        error_threshold = 5.0  # 5 Î¼s é˜ˆå€¼
        if rmse > error_threshold:
            warnings_list.append(f"âš ï¸  RMSE ({rmse:.2f} Î¼s) è¶…è¿‡é˜ˆå€¼ {error_threshold} Î¼s")
            print(f"  âš ï¸  RMSE è¶…è¿‡é˜ˆå€¼")
        else:
            print(f"  âœ“ é¢„æµ‹è¯¯å·®åœ¨å¯æ¥å—èŒƒå›´å†…")
        
        # ========== æ£€æŸ¥ 3: å¼‚å¸¸é…å¯¹ ==========
        large_errors = np.where(np.abs(errors) > 10.0)[0]  # è¯¯å·® > 10 Î¼s çš„é…å¯¹
        
        print(f"\n[æ£€æŸ¥ 3] å¼‚å¸¸é…å¯¹ï¼ˆè¯¯å·® > 10 Î¼sï¼‰")
        if len(large_errors) > 0:
            warnings_list.append(f"âš ï¸  {len(large_errors)} ä¸ªé…å¯¹çš„é¢„æµ‹è¯¯å·®è¿‡å¤§")
            print(f"  å‘ç° {len(large_errors)} ä¸ªå¼‚å¸¸é…å¯¹:")
            print(f"  {'é…å¯¹ç´¢å¼•':<10} {'æµ‹é‡(Î¼s)':<12} {'é¢„æµ‹(Î¼s)':<12} {'è¯¯å·®(Î¼s)':<12}")
            print(f"  {'-'*50}")
            for idx in large_errors[:5]:  # åªæ˜¾ç¤ºå‰ 5 ä¸ª
                print(f"  {idx:<10} {r_measured[idx, 0]:>10.4f}  {r_pred[idx, 0]:>10.4f}  {errors[idx]:>10.4f}")
            if len(large_errors) > 5:
                print(f"  ... è¿˜æœ‰ {len(large_errors) - 5} ä¸ªï¼ˆè¯¦è§æŠ¥å‘Šæ–‡ä»¶ï¼‰")
        else:
            print(f"  âœ“ æ— å¼‚å¸¸é…å¯¹")
        
        # ========== æ£€æŸ¥ 4: ç‰©ç†çº¦æŸæ»¡è¶³æƒ…å†µ ==========
        shared_counts = np.sum(M, axis=1, keepdims=True)
        physical_lower = shared_counts * self.unit_delay
        violations = r_pred < physical_lower * 0.95  # å…è®¸ 5% è¯¯å·®
        
        print(f"\n[æ£€æŸ¥ 4] ç‰©ç†çº¦æŸæ»¡è¶³æƒ…å†µ")
        num_violations = np.sum(violations)
        if num_violations > 0:
            warnings_list.append(f"âš ï¸  {num_violations} ä¸ªé…å¯¹çš„é¢„æµ‹å»¶è¿Ÿä½äºç‰©ç†ä¸‹ç•Œ")
            print(f"  âš ï¸  {num_violations} ä¸ªé…å¯¹è¿åç‰©ç†çº¦æŸ")
        else:
            print(f"  âœ“ æ‰€æœ‰é¢„æµ‹å»¶è¿Ÿæ»¡è¶³ç‰©ç†çº¦æŸ")
        
        # ========== æ€»ç»“ ==========
        print(f"\n{'='*70}")
        if len(warnings_list) == 0:
            print(f"âœ… éªŒè¯é€šè¿‡ï¼šæœªå‘ç°æ˜æ˜¾é—®é¢˜")
        else:
            print(f"âš ï¸  å‘ç° {len(warnings_list)} ä¸ªæ½œåœ¨é—®é¢˜:")
            for i, warning in enumerate(warnings_list, 1):
                print(f"  {i}. {warning}")
        print(f"{'='*70}\n")
        
        return {
            'warnings': warnings_list,
            'outliers_low': outliers_low,
            'outliers_high': outliers_high,
            'rmse': rmse,
            'mae': mae,
            'max_error': max_error,
            'large_error_pairs': large_errors,
            'constraint_violations': np.where(violations)[0]
        }
    
    def _save_diagnostic_report(self, diagnostic_info, early_stopped, final_step):
        """ä¿å­˜è¯Šæ–­æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        with open(self.diagnostic_report_path, 'w', encoding='utf-8') as f:
            f.write(f"èŠ‚ç‚¹å»¶è¿Ÿæ±‚è§£è¯Šæ–­æŠ¥å‘Š\n")
            f.write(f"{'='*70}\n")
            f.write(f"æ‹“æ‰‘ç¼–å·: {self.topo_num}\n")
            f.write(f"æ¢æµ‹æ¬¡æ•°: {self.prob_num}\n")
            f.write(f"ä¼˜åŒ–å®Œæˆæ­¥æ•°: {final_step}\n")
            f.write(f"æ—©åœçŠ¶æ€: {'æ˜¯' if early_stopped else 'å¦'}\n")
            f.write(f"\n{'='*70}\n")
            f.write(f"éªŒè¯ç»“æœ\n")
            f.write(f"{'='*70}\n\n")
            
            f.write(f"RMSE: {diagnostic_info['rmse']:.4f} Î¼s\n")
            f.write(f"MAE: {diagnostic_info['mae']:.4f} Î¼s\n")
            f.write(f"æœ€å¤§è¯¯å·®: {diagnostic_info['max_error']:.4f} Î¼s\n\n")
            
            if len(diagnostic_info['warnings']) > 0:
                f.write(f"è­¦å‘Šä¿¡æ¯:\n")
                for i, warning in enumerate(diagnostic_info['warnings'], 1):
                    f.write(f"  {i}. {warning}\n")
            else:
                f.write(f"âœ“ æ— è­¦å‘Š\n")
            
            f.write(f"\n{'='*70}\n")
            f.write(f"è¯¦ç»†ä¿¡æ¯\n")
            f.write(f"{'='*70}\n\n")
            
            if len(diagnostic_info['outliers_low']) > 0:
                f.write(f"ä½å»¶è¿Ÿå¼‚å¸¸èŠ‚ç‚¹: {diagnostic_info['outliers_low'].tolist()}\n")
            
            if len(diagnostic_info['outliers_high']) > 0:
                f.write(f"é«˜å»¶è¿Ÿå¼‚å¸¸èŠ‚ç‚¹: {diagnostic_info['outliers_high'].tolist()}\n")
            
            if len(diagnostic_info['large_error_pairs']) > 0:
                f.write(f"\nå¤§è¯¯å·®é…å¯¹ (è¯¯å·® > 10 Î¼s):\n")
                for idx in diagnostic_info['large_error_pairs']:
                    f.write(f"  é…å¯¹ {idx}\n")
        
        print(f"è¯Šæ–­æŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.diagnostic_report_path}")
    
    def _plot_convergence(self, loss_history):
        """ç»˜åˆ¶æ”¶æ•›æ›²çº¿"""
        plt.figure(figsize=(12, 6))
        
        # ç»˜åˆ¶æŸå¤±æ›²çº¿
        plt.plot(loss_history, linewidth=2, color='#2E86AB')
        plt.xlabel("Iteration", fontsize=14, fontweight='bold')
        plt.ylabel("Loss", fontsize=14, fontweight='bold')
        plt.title(f"Node Delay Optimization Convergence\n(Topo {self.topo_num}, Prob {self.prob_num})", 
                  fontsize=16, fontweight='bold')
        plt.grid(True, alpha=0.3, linestyle='--')
        plt.yscale('log')
        
        # æ ‡æ³¨æœ€ç»ˆæŸå¤±
        final_loss = loss_history[-1]
        plt.axhline(y=final_loss, color='red', linestyle='--', linewidth=1, alpha=0.7)
        plt.text(len(loss_history) * 0.7, final_loss * 1.5, 
                 f'Final Loss: {final_loss:.6f}', 
                 fontsize=12, color='red')
        
        plt.tight_layout()
        plt.savefig(self.convergence_png_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"æ”¶æ•›æ›²çº¿å·²ä¿å­˜åˆ°: {self.convergence_png_path}")
    
    def _print_node_delays(self, d):
        """æ‰“å°èŠ‚ç‚¹å»¶è¿Ÿè¯¦æƒ…"""
        print(f"\n{'='*70}")
        print(f"èŠ‚ç‚¹å»¶è¿Ÿè¯¦æƒ…")
        print(f"{'='*70}")
        print(f"{'èŠ‚ç‚¹':<8} {'å»¶è¿Ÿ(Î¼s)':<14} {'ä¸ç†è®ºå€¼å·®å¼‚(Î¼s)':<22} {'çŠ¶æ€':<10}")
        print(f"{'-'*70}")
        
        for i, delay in enumerate(d.flatten()):
            diff = delay - self.unit_delay
            
            # åˆ¤æ–­çŠ¶æ€
            if delay < 0.5 * self.unit_delay:
                status = "âš ï¸  è¿‡ä½"
            elif delay > 5.0 * self.unit_delay:
                status = "âš ï¸  è¿‡é«˜"
            else:
                status = "âœ“ æ­£å¸¸"
            
            print(f"{i:<8} {delay:>12.4f}    {diff:>18.4f}      {status:<10}")
        
        print(f"{'='*70}\n")


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # æµ‹è¯•è·¯å¾„
    M_file = "/home/retr0/Project/TopologyObfu/CritiPro/topo_deployment/data/input_file/M.txt"
    r_file = "/home/retr0/Project/TopologyObfu/CritiPro/topo_deployment/data/input_file/r.txt"
    
    solver = NodeDelaySolver(
        topo_num="topo_1",
        prob_num=500,
        unit_delay=12.0,
        lr=0.01,
        max_iter=3000,
        early_stop_patience=100,
        early_stop_threshold=1e-5
    )
    
    d, r_pred = solver.solve(M_file, r_file)
    
    print("\n" + "="*70)
    print("ä¼˜åŒ–åçš„èŠ‚ç‚¹å»¶è¿Ÿ:")
    print("="*70)
    print(d)
    print("\n" + "="*70)
    print("é¢„æµ‹çš„å»¶è¿Ÿå‘é‡:")
    print("="*70)
    print(r_pred)
