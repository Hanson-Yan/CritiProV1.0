import numpy as np
import matplotlib.pyplot as plt
import os

class OperationMatrixAdamSolver:
    def __init__(self, topo_num,prob_num,gamma=0.0001, eta=1, alpha=0.01, max_iter=5000, tol=1e-4, delta_max=50, lambda_reg=0.1,lambda_grad=0.05, epsilon=0.01,deploy_reward=5):
        """
        åˆå§‹åŒ–å‚æ•°
        :param gamma: ç¨€ç–æ€§æ­£åˆ™åŒ–å‚æ•°
        :param eta: ç½šå‡½æ•°æƒ©ç½šå› å­
        :param alpha: åˆå§‹å­¦ä¹ ç‡
        :param max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
        :param tol: æ”¶æ•›é˜ˆå€¼
        :param delta_max: æœ€å¤§å»¶è¿Ÿçº¦æŸ
        :param lambda_reg: é¢å¤–çš„æ­£åˆ™åŒ–æƒé‡ï¼Œä¿è¯Pçš„æ¯ä¸€è¡Œä¸å…¨ä¸º0
        :param epsilon: æ§åˆ¶Pæ¯ä¸€è¡Œæœ€å°å€¼çš„é˜ˆå€¼
        """
        self.deploy_reward = deploy_reward
        self.gamma = gamma
        self.eta = eta
        self.alpha = alpha
        self.max_iter = max_iter
        self.tol = tol
        self.delta_max = delta_max
        self.lambda_reg = lambda_reg  # é¢å¤–çš„æ­£åˆ™åŒ–é¡¹æƒé‡ 
        self.lambda_grad = lambda_grad # é¢å¤–çš„ä¿è¯Pr>=ræ¢¯åº¦æ§åˆ¶
        self.epsilon = epsilon  # æ§åˆ¶æœ€å°è¡ŒèŒƒæ•°é˜ˆå€¼
        self.operation_matrix_path = f"/home/retr0/Project/TopologyObfu/CritiPro/topo_deployment/data/output_file/{topo_num}_{prob_num}_operation_matrix.txt"
        self.deployment_vector_path = f"/home/retr0/Project/TopologyObfu/CritiPro/topo_deployment/data/output_file/{topo_num}_{prob_num}_deployment_vector.txt"
        self.convergence_png_path=f"/home/retr0/Project/TopologyObfu/CritiPro/topo_deployment/data/output_file/{topo_num}_{prob_num}_convergence_curve.png"

    @staticmethod
    def read_matrix(file_path):
        return np.loadtxt(file_path, dtype=float)

    @staticmethod
    def read_vector(file_path):
        return np.loadtxt(file_path, dtype=float).reshape(-1, 1)

    def penalty_function(self, P, M, F, r):
        """
        è®¡ç®—ä¼˜åŒ–ç›®æ ‡ H(F) = ||PM - F||_2 + Î³||Pr - r||_1 + Î·Q(P) +  Î» âˆ‘ max(0, Îµ - ||P_i||_1) + Î»2 âˆ‘ max(0, r - Pr)^2
        """
        PM = P @ M
        Pr = P @ r
        term1 = np.linalg.norm(PM - F, ord=2)
        term2 = self.gamma * np.linalg.norm(Pr - r, ord=1)

        # è®¡ç®—ç½šå‡½æ•°é¡¹
        delta = Pr - r
        term3 = np.sum(np.maximum(0, delta - self.delta_max) ** 2) + np.sum(np.maximum(0, -delta) ** 2)

        # ç¡®ä¿ P æ¯ä¸€è¡Œçš„ L1 èŒƒæ•°ä¸ä½äº Îµ
        term4 = np.sum(np.maximum(0, self.epsilon - np.sum(np.abs(P), axis=1)))

        # é¢å¤–çº¦æŸ Pr >= r
        term5 = np.sum(np.maximum(0, r - Pr) ** 2)  # è®© Pr å°½å¯èƒ½ >= r

        return term1 + term2 + self.eta * term3 + self.lambda_reg * term4 + self.lambda_grad * term5
    
    # def penalty_function(self, P, M, F, r, iteration=0):
    #     PM = P @ M
    #     Pr = P @ r

    #     # ä¿æŒä¸»é€»è¾‘ä¸å˜ï¼Œåªè°ƒæ•´æ•°å€¼èŒƒå›´
    #     SCALE_TERM1 = 100 / max(1, M.size)
    #     SCALE_TERM2 = 100 / max(1, r.size)

    #     term1 = np.linalg.norm(PM - F, ord=2) * SCALE_TERM1
    #     # term2 = self.gamma * np.linalg.norm(Pr - r, ord=1) * SCALE_TERM2
    #     # diff = np.abs(Pr - r)
    #     diff = Pr - r
    #     soft_mask = np.tanh((diff - 1.0) * 3)
    #     term2 = self.gamma * np.sum(np.maximum(0, soft_mask))

    #     delta = Pr - r
    #     term3 = (
    #         np.sum(np.maximum(0, delta - self.delta_max) ** 2) +
    #         np.sum(np.maximum(0, -delta) ** 2)
    #     )
    #     term3 = np.clip(term3, 0, 10)

    #     term4 = np.sum(np.maximum(0, self.epsilon - np.sum(np.abs(P), axis=1)))

    #     term5 = np.sum(np.maximum(0, r - Pr) ** 2)
    #     term5 = np.clip(term5, 0, 10)

    #     # æ¿€åŠ±é¡¹ï¼šé¼“åŠ±æ˜æ˜¾æ‰°åŠ¨ï¼ˆå¦‚ > 1ï¼‰
    #     deployment_gain = np.sum(np.maximum(0, Pr - r - 1))  # åªæœ‰æ‰°åŠ¨è¶…è¿‡ 1 çš„éƒ¨åˆ†æ‰è®¡å…¥
    #     term6 = -self.deploy_reward * deployment_gain  # è´Ÿå·æ˜¯â€œå¥–åŠ±é¡¹â€ï¼Œè¶Šå¤§è¶Šå¥½
    #     return term1 + term2 + self.eta * term3 + self.lambda_reg * term4 + self.lambda_grad * term5 + term6


    def gradient(self, P, M, F, r):
        """
        è®¡ç®—ç›®æ ‡å‡½æ•°å¯¹ P çš„æ¢¯åº¦
        """
        PM = P @ M
        Pr = P @ r
        grad_P = 2 * (PM - F) @ M.T + self.gamma * np.sign(Pr - r) @ r.T

        # è®¡ç®—ç½šå‡½æ•°æ¢¯åº¦é¡¹
        delta = Pr - r
        penalty_grad = 2 * np.maximum(0, delta - self.delta_max) + 2 * np.maximum(0, -delta)
        grad_P += self.eta * penalty_grad

        # è®¡ç®—æ–°æ­£åˆ™é¡¹çš„æ¢¯åº¦
        row_norms = np.sum(np.abs(P), axis=1, keepdims=True)
        grad_P += self.lambda_reg * (-1) * (row_norms < self.epsilon)

        # å¯¹ Pr ä½äº r çš„éƒ¨åˆ†æ–½åŠ æ¢¯åº¦
        constraint_grad = -2 * np.maximum(0, r - Pr) @ r.T  # åªå¯¹ Pr < r çš„éƒ¨åˆ†ç”Ÿæ•ˆ
        grad_P += self.lambda_grad * constraint_grad  # åŠ å…¥æ¢¯åº¦

        return grad_P

    def adam_optimizer(self, M, F, r, lr=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):
        """
        ä½¿ç”¨ Adam ä¼˜åŒ–å™¨è¿›è¡Œä¼˜åŒ–ï¼Œå¹¶ç¡®ä¿ P æ¯ä¸€è¡Œè‡³å°‘æœ‰ä¸€ä¸ªéé›¶å…ƒç´ 
        """
        n = M.shape[0]
        P = np.random.uniform(0.1, 1, size=(n, n))
        m = np.zeros_like(P)
        v = np.zeros_like(P)
        t = 0

        loss_values = []
        count_loss = 0
        for iteration in range(self.max_iter):
            t += 1
            grad_P = self.gradient(P, M, F, r)

            # Adam å…¬å¼
            m = beta1 * m + (1 - beta1) * grad_P
            v = beta2 * v + (1 - beta2) * grad_P ** 2
            m_hat = m / (1 - beta1 ** t)
            v_hat = v / (1 - beta2 ** t)

            # æ›´æ–° P
            P -= lr * m_hat / (np.sqrt(v_hat) + epsilon)
            P = np.clip(P, 1e-3, 1)  # ç¡®ä¿ P åœ¨åˆç†èŒƒå›´å†…
             # è®°å½•æ¯æ¬¡è¿­ä»£çš„ç›®æ ‡å‡½æ•°å€¼
            loss_value = self.penalty_function(P, M, F, r)
            loss_values.append(loss_value)
            if len(loss_values)>1 :
                if loss_values[-1]>loss_values[-2]:
                    print(f"-------------------learning rate 1/2--------------------")
                    lr=lr*0.5
                elif loss_values[-1]==loss_values[-2]:
                    count_loss+=1
                    if count_loss>10:
                        print(f"-------------------learning rate ++--------------------")
                        lr+=lr
                        count_loss=0
            # loss_values.append(self.penalty_function(P, M, F, r))
            # æ‰“å°æ¯æ¬¡è¿­ä»£çš„ä¿¡æ¯
            if iteration % 10 == 0:  # æ¯10æ¬¡è¿­ä»£æ‰“å°ä¸€æ¬¡
                print(f"Iteration {iteration}, Objective Function Value: {loss_value:.6f}, Learning Rate: {lr:.6f}")

            if np.linalg.norm(grad_P) < self.tol:
                print(f"æ”¶æ•›äºç¬¬ {iteration} æ¬¡è¿­ä»£ï¼")
                break

        # P = self.post_process_P(P)  # è¿›è¡Œåå¤„ç†
        Pr = P @ r
        np.savetxt(self.operation_matrix_path, P, fmt="%.6f")
        print(f"ç”Ÿæˆçš„æ“ä½œçŸ©é˜µå·²ä¿å­˜è‡³\n{self.operation_matrix_path}")
        np.savetxt(self.deployment_vector_path, Pr, fmt="%.6f")
        print(f"ç”Ÿæˆçš„éƒ¨ç½²å‘é‡å·²ä¿å­˜è‡³\n{self.deployment_vector_path}")
        # ç»˜åˆ¶æ”¶æ•›æ›²çº¿
        plt.plot(loss_values)
        plt.xlabel('Iteration')
        plt.ylabel('Objective Function Value')
        plt.title('Convergence Curve')
        plt.grid(False)
        plt.savefig(self.convergence_png_path)  # ä¿å­˜å›¾åƒ
        print(f"ç”Ÿæˆçš„æ”¶æ•›æ›²çº¿å·²ä¿å­˜è‡³\n{self.convergence_png_path}")
        # plt.show()
        return P, Pr

    # def enforce_pr_constraint(self,P, r, delta_min=1e-3):
    #     Pr = P @ r
    #     for i in range(P.shape[0]):
    #         if Pr[i] < r[i,0]:
    #             row = P[i, :].copy()
    #             current = row @ r[:,0]
    #             scale = (r[i,0] + delta_min) / current
    #             P[i, :] = np.clip(row * scale, 1e-3, 1)
    #     return P
    
    # def adam_optimizer(self, M, F, r, base_lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, warmup_steps=50):
    #     n = M.shape[0]

    #     # å­¦ä¹ ç‡éšè§„æ¨¡è‡ªé€‚åº”ç¼©æ”¾
    #     scaled_lr = base_lr * (min(n, 100) / 100.0)

    #     # def get_lr(t, T_max):
    #     #     return max(1e-4, scaled_lr * (1 - t / T_max)) 
    #     def get_lr(t, max_iter, base_lr):
    #         return base_lr * (1 + np.cos(np.pi * t / max_iter)) / 2

    #     P = np.random.uniform(0.1, 1, size=(n, n))
    #     m = np.zeros_like(P)
    #     v = np.zeros_like(P)
    #     t = 0
    #     loss_values = []

    #     for iteration in range(self.max_iter):
    #         t += 1
    #         lr = get_lr(t, self.max_iter,self.alpha)

    #         grad_P = self.gradient(P, M, F, r)

    #         # Adam æ›´æ–°
    #         m = beta1 * m + (1 - beta1) * grad_P
    #         v = beta2 * v + (1 - beta2) * grad_P ** 2
    #         m_hat = m / (1 - beta1 ** t)
    #         v_hat = v / (1 - beta2 ** t)

    #         P -= lr * m_hat / (np.sqrt(v_hat) + epsilon)

    #         # P = self.enforce_pr_constraint(P,r)

    #         P = np.clip(P, 1e-3, 1)

    #         # âœ… ä¼ å…¥å½“å‰è¿­ä»£ï¼Œå¯ç”¨åŠ¨æ€æ­£åˆ™é¡¹
    #         loss_value = self.penalty_function(P, M, F, r, iteration=iteration)
    #         loss_values.append(loss_value)

    #         if iteration % 10 == 0:
    #             print(f"Iteration {iteration}, Objective: {loss_value:.6f}, Learning Rate: {lr:.6f}")

    #         if np.linalg.norm(grad_P) < self.tol:
    #             print(f"æ”¶æ•›äºç¬¬ {iteration} æ¬¡è¿­ä»£ï¼")
    #             break

    #     Pr = P @ r
    #     np.savetxt(self.operation_matrix_path, P, fmt="%.6f")
    #     print(f"æ“ä½œçŸ©é˜µä¿å­˜è‡³\n{self.operation_matrix_path}")
    #     np.savetxt(self.deployment_vector_path, Pr, fmt="%.6f")
    #     print(f"éƒ¨ç½²å‘é‡ä¿å­˜è‡³\n{self.deployment_vector_path}")

    #     # æ”¶æ•›æ›²çº¿ç»˜å›¾
    #     plt.plot(loss_values)
    #     plt.xlabel('Iteration')
    #     plt.ylabel('Objective Function Value')
    #     plt.title('Convergence Curve')
    #     plt.grid(False)
    #     plt.savefig(self.convergence_png_path)
    #     print(f"æ”¶æ•›æ›²çº¿å·²ä¿å­˜è‡³\n{self.convergence_png_path}")

    #     return P, Pr

    # def post_process_P(self, P):
    #     """
    #     ä¿®æ­£ Pï¼Œé˜²æ­¢æŸäº›è¡Œå…¨ä¸º 0
    #     """
    #     for i in range(P.shape[0]):
    #         if np.all(P[i, :] == 0):  # å¦‚æœæŸè¡Œå…¨ 0
    #             P[i, i] = 1  # åœ¨å¯¹è§’çº¿ä¸Šç½® 1

    #     return P



    def solve(self, M_file, F_file, r_file, method="adam"):
        """
        è¯»å–æ•°æ®å¹¶æ±‚è§£ P
        """
        M = self.read_matrix(M_file)
        F = self.read_matrix(F_file)
        r = self.read_vector(r_file)

            # ğŸŒŸ æ‹“æ‰‘è§„æ¨¡å½’ä¸€åŒ–æ­£åˆ™é¡¹ï¼ˆåŸºå‡†100ä¸ªèŠ‚ç‚¹ï¼‰
        n = M.shape[0]
        scale_factor = 100 / max(n, 1)
        self.gamma *= scale_factor
        self.lambda_reg *= scale_factor
        self.lambda_grad *= scale_factor

        if method == "adam":
            P, Pr = self.adam_optimizer(M, F, r, lr=self.alpha)
            # P, Pr = self.adam_optimizer(M, F, r, base_lr=self.alpha)
            return P, Pr
        else:
            return None
        # P, Pr = self.adam_optimizer(M, F, r)
        # return P, Pr
